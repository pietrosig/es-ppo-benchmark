{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jk0ho9cW_W6"
      },
      "source": [
        "# **Bipedal walker solved with EVOLUTION STRATEGIES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoiHduvEXMEr"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfJ5MXXwPGAw"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3tYy71yXRAK"
      },
      "source": [
        "# **AgentNN definition**\n",
        "AgentNN will be the network that outputs the action in funcion of the states. It will be the same one for the PPO and for the ES."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk1M81zGeO3o"
      },
      "outputs": [],
      "source": [
        "class AgentNN(nn.Module): #its the Agent network in the ES, and the PolicyNetwork in the PPO\n",
        "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
        "        super(AgentNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_inputs, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, n_outputs)\n",
        "\n",
        "    def loadFromTensors(self, W1, W2, b1, b2):\n",
        "        self.fc1.weight = nn.Parameter(W1)\n",
        "        self.fc2.weight = nn.Parameter(W2)\n",
        "        self.fc1.bias = nn.Parameter(b1)\n",
        "        self.fc2.bias = nn.Parameter(b2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.fc1(x))\n",
        "        x = F.tanh(self.fc2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mwaRheHWtTi"
      },
      "source": [
        "# **Agent, breeding and usefull funtion definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMQi5h18O9ZF"
      },
      "outputs": [],
      "source": [
        "def glorot_uniform(n_inputs,n_outputs,multiplier=1.0): #We used this type of weight inizialization to make the variance costant in the network (Xavier init)\n",
        "    glorot = multiplier*np.sqrt(6.0/(n_inputs+n_outputs))\n",
        "    return np.random.uniform(-glorot,glorot,size=(n_inputs,n_outputs))\n",
        "\n",
        "def softmax(scores,temp=5.0):\n",
        "    exp = np.exp(np.array(scores)/temp)\n",
        "    return exp/exp.sum()\n",
        "\n",
        "class Agent(object):\n",
        "    #Activation= Tanh\n",
        "    def __init__(self, n_inputs, n_hidden, n_outputs, mutate_rate=.05, init_multiplier=1.0):\n",
        "        ''' Create agent's brain '''\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_outputs = n_outputs\n",
        "        self.mutate_rate = mutate_rate\n",
        "        self.init_multiplier = init_multiplier\n",
        "        self.network = {'Layer 1' : glorot_uniform(n_inputs, n_hidden,init_multiplier), #(25,512)\n",
        "                        'Bias 1'  : np.zeros((1,n_hidden)),\n",
        "                        'Layer 2' : glorot_uniform(n_hidden, n_outputs,init_multiplier), #(512,4)\n",
        "                        'Bias 2'  : np.zeros((1,n_outputs))}\n",
        "\n",
        "    def act(self, state): #use the network to make an action\n",
        "        if type(state) == tuple:\n",
        "            state = state[0]\n",
        "        if(state.shape[0] != 1):\n",
        "            state = state.reshape(1,-1)\n",
        "        net = self.network\n",
        "        layer_one = np.tanh(np.matmul(state,net['Layer 1']) + net['Bias 1'])\n",
        "        layer_two = np.tanh(np.matmul(layer_one, net['Layer 2']) + net['Bias 2'])\n",
        "        return layer_two[0]\n",
        "\n",
        "    def __add__(self, another): # overload the + operator for the breeding\n",
        "        child = Agent(self.n_inputs, self.n_hidden, self.n_outputs, self.mutate_rate, self.init_multiplier)\n",
        "        for key in child.network:\n",
        "            n_inputs,n_outputs = child.network[key].shape\n",
        "            mask = np.random.choice([0,1],size=child.network[key].shape,p=[.5,.5])\n",
        "            random = glorot_uniform(mask.shape[0],mask.shape[1]) #random weights initialized with glorot\n",
        "            child.network[key] = np.where(mask==1,self.network[key],another.network[key]) #returns indices where mask =1\n",
        "            mask = np.random.choice([0,1],size=child.network[key].shape,p=[1-self.mutate_rate,self.mutate_rate]) #selects 0 with 1-mutationrate prob, 1 with mut. rate prob\n",
        "            child.network[key] = np.where(mask==1,child.network[key]+random,child.network[key]) # updates child networks layers with weights=child.network[key]+random when mask 1\n",
        "        return child\n",
        "\n",
        "def run_trial(env,agent,verbose=False): #an agent performs 3 episodes of the env\n",
        "    totals = []\n",
        "    for _ in range(3):\n",
        "        state = env.reset()\n",
        "        if verbose: env.render()\n",
        "        total = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            #print(env.step(agent.act(state)))\n",
        "            state, reward, _, done = env.step(agent.act(state))\n",
        "            if reward >= 100:\n",
        "                env.render()\n",
        "#             if verbose: env.render()\n",
        "            total += reward\n",
        "        totals.append(total)\n",
        "    return sum(totals)/3.0\n",
        "\n",
        "def next_generation(env,population,scores,temperature): #breeds a new generation of agents\n",
        "    scores, population =  zip(*sorted(zip(scores,population),reverse=True)) #sort scores and population w.r.t. scores.\n",
        "    #select the first 25% agents and mark as children\n",
        "    children = list(population[:len(population)//4])\n",
        "    #fill the remaining children with the best of parents.\n",
        "    #A random sample is generated from population,with probabilities returned from softmax.\n",
        "    #create 2 times the size of agents remaining after 25% children are removed.\n",
        "    parents = list(np.random.choice(population,size=2*(len(population)-len(children)),p=softmax(scores,temperature)))\n",
        "    #Breed between 2 Agent's from the above list and add it to the children list.\n",
        "    children = children + [parents[i]+parents[i+1] for i in range(0,len(parents)-1,2)]\n",
        "    #run the children agents and return children agents and their scores.\n",
        "    scores = [run_trial(env,agent) for agent in children]\n",
        "\n",
        "    return children,scores\n",
        "\n",
        "\n",
        "def update_plot(graph, new_data):\n",
        "    graph.set_xdata(np.append(graph.get_xdata(), new_data[0]))\n",
        "    graph.set_ydata(np.append(graph.get_ydata(), new_data[1]))\n",
        "    plt.draw()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqnczKebedr4"
      },
      "source": [
        "# **TRANING LOOP**\n",
        "Hyperparameters intialization and traning loop. When there is a new best model, it's saved. We track the score of the best model of each generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq4bwasveUL2"
      },
      "outputs": [],
      "source": [
        "######## TRANING ##########\n",
        "graph = plt.plot([],[])\n",
        "# Setup environment\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "np.random.seed(0)\n",
        "genlist=[]\n",
        "rewardlist=[]\n",
        "# network params\n",
        "n_inputs = env.observation_space.shape[0] # 24 observations\n",
        "n_actions = env.action_space.shape[0] # 4 actions\n",
        "n_hidden = 512\n",
        "multiplier = 5\n",
        "\n",
        "# Population params\n",
        "pop_size = 50\n",
        "mutate_rate = .1\n",
        "softmax_temp = 5.0\n",
        "\n",
        "# Training\n",
        "n_generations = 80\n",
        "#create agents(as per population size)\n",
        "population = [Agent(n_inputs,n_hidden,n_actions,mutate_rate,multiplier) for i in range(pop_size)]\n",
        "BESTMODEL = population[0]\n",
        "\n",
        "\n",
        "#run all agents in the population\n",
        "scores = [run_trial(env,agent) for agent in population]\n",
        "#choose the best agent from the above trial and store it as best agent.\n",
        "best = [deepcopy(population[np.argmax(scores)])]\n",
        "#create new generation and repeat for n generations\n",
        "a = time.time()\n",
        "bestScore = -10000000000\n",
        "for generation in range(n_generations):\n",
        "\n",
        "    #create next generation fromcurrent poulation and scores.\n",
        "    population,scores = next_generation(env,population, scores,softmax_temp)\n",
        "    best.append(deepcopy(population[np.argmax(scores)]))\n",
        "    print(\"Generation:\",generation,\"Best score:\",np.max(scores), \"Time:\", time.time()-a )\n",
        "    if np.max(scores) > bestScore:\n",
        "      BESTMODEL = population[np.argmax(scores)]\n",
        "      w1,w2,b1,b2 = torch.from_numpy(BESTMODEL.network['Layer 1']),torch.from_numpy(BESTMODEL.network['Layer 2']),torch.from_numpy(BESTMODEL.network['Bias 1']),torch.from_numpy(BESTMODEL.network['Bias 2'])\n",
        "      saveAgent = AgentNN(24,512,4)\n",
        "      saveAgent.loadFromTensors(W1=w1,W2=w2,b1=b1,b2=b2)\n",
        "      torch.save(saveAgent.state_dict(), f'/content/drive/MyDrive/RL_FINAL/bestAgentES.pt') # if the best model of the new generation is best ever we save it\n",
        "      bestScore = np.max(scores)\n",
        "      print(f'new best model saved{bestScore}')\n",
        "    f =  open('/content/drive/MyDrive/RL_FINAL/REWARDS_ES.json', 'a')\n",
        "    json.dump({\"generation\":generation,\"time\":time.time()-a,\"best_reward\":np.max(scores)},f)\n",
        "    f.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
